{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Unsupervised Shortcut Learning Detection and Mitigation in Transformers\n",
    "\n",
    "This is the official code for the paper \"Efficient Unsupervised Shortcut Learning Detection and Mitigation in Transformers\" by Lukas Kuhn, Sari Sadiya, Joerg Schloetterer, Christin Seifert, Gemma Roig.\n",
    "\n",
    "Please contact Lukas Kuhn (lukas.kuhn@dkfz-heidelberg.de) for any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import replicate\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from surgeon_pytorch import Inspect, Extract\n",
    "from torch import nn\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "from tqdm import tqdm\n",
    "import openvino_xai as xai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the ISIC dataset for this example. Please download and sort the ISIC dataset into the following folders:\n",
    "\n",
    "- ./data/isic/val_wo_patches/\n",
    "- ./data/isic/val_w_patches/\n",
    "- ./data/isic/test_wo_patches/\n",
    "- ./data/isic/test_w_patches/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "mean_ds = [0.485, 0.456, 0.406]\n",
    "std_ds = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the images to a fixed size\n",
    "    transforms.ToTensor(),  # Convert the images to tensors\n",
    "    transforms.Normalize(mean=mean_ds, std=std_ds)  # Normalize the images\n",
    "])\n",
    "\n",
    "# Create the ImageFolder dataset\n",
    "val_dataset_wo_patches = datasets.ImageFolder(root='./data/isic/val_wo_patches/', transform=transform)\n",
    "val_dataset_w_patches = datasets.ImageFolder(root='./data/isic/val_w_patches/', transform=transform)\n",
    "\n",
    "concatenated_dataset = ConcatDataset([val_dataset_wo_patches, val_dataset_w_patches])\n",
    "\n",
    "test_dataset_wo_patches = datasets.ImageFolder(root='./data/isic/test_wo_patches/', transform=transform)\n",
    "test_dataset_w_patches = datasets.ImageFolder(root='./data/isic/test_w_patches/', transform=transform)\n",
    "\n",
    "test_concatenated_dataset = ConcatDataset([test_dataset_wo_patches, test_dataset_w_patches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running this code on a MacBook Pro with an M3 chip and 16GB hence the usage of MPS. It can easily be adapted to other devices. The following code loads the pre-trained ViT model and modifies the final layer for the dataset. It also loads the pre-trained model and inserts the XAI layer, to get the saliency maps, which we only use for visualization during our research and not for the shortcut detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = f\"vit_isic_{seed}\"\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained ViT model\n",
    "model = vit_b_16(weights='DEFAULT')\n",
    "\n",
    "# Modify the final layer for the dataset\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model.load_state_dict(torch.load(f\"models/{MODEL_NAME}.pth\", map_location=device, weights_only=True))\n",
    "\n",
    "inspect = Inspect(model, layer=\"encoder.layers.encoder_layer_11\")\n",
    "\n",
    "model_xai: torch.nn.Module = xai.insert_xai(model, xai.Task.CLASSIFICATION).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the dataset into two subsets, one with patches and one without and also based on the target class for easier identification of the worst group. This knowledge is only used for evaluation and not for the shortcut detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subset_by_target(ds, target_class):\n",
    "    indices = [i for i, (_, target) in enumerate(ds) if target == target_class]\n",
    "\n",
    "    subset = torch.utils.data.Subset(ds, indices)\n",
    "    return subset\n",
    "\n",
    "def inference_model(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for img, label in tqdm(loader):\n",
    "        label = label.to(device)\n",
    "        out = model(img.to(device))\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "\n",
    "        correct += (pred == label).sum().item()\n",
    "        total += img.size(0)\n",
    "\n",
    "    print(f\"Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "test_w_patches_cls_0 = load_subset_by_target(test_dataset_w_patches, 0)\n",
    "test_w_patches_cls_1 = load_subset_by_target(test_dataset_w_patches, 1)\n",
    "\n",
    "test_wo_patches_cls_0 = load_subset_by_target(test_dataset_wo_patches, 0)\n",
    "test_wo_patches_cls_1 = load_subset_by_target(test_dataset_wo_patches, 1)\n",
    "\n",
    "test_w_patches_cls_0_loader = torch.utils.data.DataLoader(test_w_patches_cls_0, batch_size=16, shuffle=False)\n",
    "test_w_patches_cls_1_loader = torch.utils.data.DataLoader(test_w_patches_cls_1, batch_size=16, shuffle=False)\n",
    "test_wo_patches_cls_0_loader = torch.utils.data.DataLoader(test_wo_patches_cls_0, batch_size=16, shuffle=False)\n",
    "test_wo_patches_cls_1_loader = torch.utils.data.DataLoader(test_wo_patches_cls_1, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "inference_model(model, test_w_patches_cls_0_loader)\n",
    "inference_model(model, test_w_patches_cls_1_loader)\n",
    "inference_model(model, test_wo_patches_cls_0_loader)\n",
    "inference_model(model, test_wo_patches_cls_1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_layer = model.encoder.layers[-1].self_attention\n",
    "\n",
    "in_proj_weight = self_attention_layer.in_proj_weight\n",
    "\n",
    "# Extract t\n",
    "# he dimensions\n",
    "embed_dim = 768\n",
    "kdim = self_attention_layer.kdim\n",
    "\n",
    "# Extract the weight matrix for the keys (Wk) from the combined weight matrix\n",
    "W_K = in_proj_weight[embed_dim:2*embed_dim, :]\n",
    "\n",
    "conv_out_model = Extract(model, node_out=\"encoder.dropout\") \n",
    "conv_out_model.eval()\n",
    "conv_out_model = conv_out_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the keys from the model for each image in the dataset following _Bolya et al._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keys(dataset):\n",
    "    keys = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        out = conv_out_model(dataset[i][0].unsqueeze(0).to(device))\n",
    "\n",
    "        ln_1 = out[:,1:,:]\n",
    "        k = ln_1 @ W_K.T\n",
    "\n",
    "        k = k.reshape(1, 196, 12, 64)\n",
    "        k_mean = k.mean(dim=2)\n",
    "\n",
    "        keys.append(k_mean.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    return np.array(keys)\n",
    "\n",
    "# Example usage:\n",
    "keys_w_patches = extract_keys(val_dataset_w_patches)\n",
    "keys_wo_patches = extract_keys(val_dataset_wo_patches)\n",
    "\n",
    "test_keys_w_patches = extract_keys(test_dataset_w_patches)\n",
    "test_keys_wo_patches = extract_keys(test_dataset_wo_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_wo_patches = []\n",
    "probs_wo_patches = []\n",
    "\n",
    "for i in range(len(val_dataset_wo_patches)):\n",
    "    probs, out = inspect(val_dataset_wo_patches[i][0].unsqueeze(0).to(device))\n",
    "    activations_wo_patches.append(out.squeeze().detach().cpu().numpy())\n",
    "    probs_wo_patches.append(probs.squeeze().detach().cpu().numpy())\n",
    "\n",
    "activations_wo_patches = np.array(activations_wo_patches)\n",
    "probs_wo_patches = np.array(probs_wo_patches)\n",
    "\n",
    "activations_w_patches = []\n",
    "probs_w_patches = []\n",
    "\n",
    "for i in range(len(val_dataset_w_patches)):\n",
    "    probs, out = inspect(val_dataset_w_patches[i][0].unsqueeze(0).to(device))\n",
    "    activations_w_patches.append(out.squeeze().detach().cpu().numpy())\n",
    "    probs_w_patches.append(probs.squeeze().detach().cpu().numpy())\n",
    "\n",
    "activations_w_patches = np.array(activations_w_patches)\n",
    "probs_w_patches = np.array(probs_w_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform PCA and K-means clustering on the activations and plot a confusion matrix to evaluate the clustering, this is also only used for evaluation and not for the shortcut detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA and K-means clustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "acts = np.concatenate([activations_wo_patches, activations_w_patches], axis=0)\n",
    "labels = np.concatenate([np.zeros(len(activations_wo_patches)), np.ones(len(activations_w_patches))], axis=0)\n",
    "\n",
    "k = acts.reshape(len(acts), 197, 768)\n",
    "k = k[:,1:]\n",
    "k = np.mean(k, axis=1)\n",
    "\n",
    "# Normalize k\n",
    "k_min = k.min()\n",
    "k_max = k.max()\n",
    "k_normalized = (k - k_min) / (k_max - k_min)\n",
    "\n",
    "# Reshape k_normalized to 2D array (required for TSNE)\n",
    "k_reshaped = k_normalized.reshape(len(k_normalized), -1)\n",
    "\n",
    "# Perform PCA with 50 components\n",
    "pca = PCA(n_components=3)\n",
    "pca_results = pca.fit_transform(k_reshaped)\n",
    "\n",
    "# Perform K-means clustering\n",
    "n_clusters = 2  # You can adjust this number based on your needs\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(pca_results)\n",
    "\n",
    "# Calculate accuracy of K-means clustering compared to true labels\n",
    "\n",
    "true_labels = labels\n",
    "\n",
    "cm = confusion_matrix(true_labels, cluster_labels)\n",
    "\n",
    "# If cluster 0 matches more with label 1, we need to flip the labels\n",
    "if cm[0][1] > cm[0][0]:\n",
    "    cluster_labels = 1 - cluster_labels\n",
    "\n",
    "# Plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix of K-means Clustering')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(true_labels, cluster_labels)\n",
    "print(f\"Clustering accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ the Brier score and class homogeneity to evaluate the clustering. This is described in detail in section 3.3.1 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# Calculate Brier score and class homogeneity for each cluster\n",
    "def calculate_metrics(probs, true_labels):\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = F.softmax(torch.tensor(probs), dim=1).numpy()\n",
    "    \n",
    "    # Brier score for each class\n",
    "    brier_scores = {\n",
    "        'class0': None,  # for samples where true label is 0\n",
    "        'class1': None   # for samples where true label is 1\n",
    "    }\n",
    "    \n",
    "    # Separate samples by true label\n",
    "    class0_mask = (true_labels == 0)\n",
    "    class1_mask = (true_labels == 1)\n",
    "    \n",
    "    # Calculate Brier score for each class\n",
    "    if np.any(class0_mask):\n",
    "        brier_scores['class0'] = np.mean((probs[class0_mask, 0] - 1) ** 2 + \n",
    "                                       (probs[class0_mask, 1] - 0) ** 2)\n",
    "    if np.any(class1_mask):\n",
    "        brier_scores['class1'] = np.mean((probs[class1_mask, 0] - 0) ** 2 + \n",
    "                                       (probs[class1_mask, 1] - 1) ** 2)\n",
    "    \n",
    "    # Add probability distribution analysis\n",
    "    prob_stats = {\n",
    "        'mean_prob_class0': np.mean(probs[:, 0]),\n",
    "        'std_prob_class0': np.std(probs[:, 0]),\n",
    "        'mean_prob_class1': np.mean(probs[:, 1]),\n",
    "        'std_prob_class1': np.std(probs[:, 1])\n",
    "    }\n",
    "    \n",
    "    class_counts = np.bincount(true_labels.astype(int))\n",
    "    homogeneity = np.max(class_counts) / len(true_labels)\n",
    "    dominant_class = np.argmax(class_counts)\n",
    "    \n",
    "    return brier_scores, homogeneity, dominant_class, prob_stats\n",
    "\n",
    "# Get probabilities and true labels for each cluster\n",
    "cluster_0_indices = np.where(cluster_labels == 0)[0]\n",
    "cluster_1_indices = np.where(cluster_labels == 1)[0]\n",
    "\n",
    "# Combine probabilities from both datasets\n",
    "all_probs = np.concatenate([probs_wo_patches, probs_w_patches], axis=0)\n",
    "all_labels = labels  # This was created earlier in your code\n",
    "\n",
    "# Calculate metrics for each cluster\n",
    "cluster_0_brier, cluster_0_homogeneity, cluster_0_dominant, cluster_0_prob_stats = calculate_metrics(\n",
    "    all_probs[cluster_0_indices], \n",
    "    all_labels[cluster_0_indices]\n",
    ")\n",
    "\n",
    "cluster_1_brier, cluster_1_homogeneity, cluster_1_dominant, cluster_1_prob_stats = calculate_metrics(\n",
    "    all_probs[cluster_1_indices], \n",
    "    all_labels[cluster_1_indices]\n",
    ")\n",
    "\n",
    "lambda1 = 1/3 \n",
    "lambda2 = 1/3\n",
    "lambda3 = 1/3\n",
    "\n",
    "def calculate_cluster_score(homogeneity, brier_scores, dominant_class):\n",
    "    # Get both Brier scores\n",
    "    brier_class0 = brier_scores['class0']\n",
    "    brier_class1 = brier_scores['class1']\n",
    "    \n",
    "    if brier_class0 is None or brier_class1 is None:\n",
    "        return 0.0\n",
    "        \n",
    "    dominant_class = 0 if brier_class0 < brier_class1 else 1\n",
    "    dominant_brier = brier_scores[f'class{dominant_class}']\n",
    "    non_dominant_brier = brier_scores[f'class{1-dominant_class}']\n",
    "    \n",
    "    return lambda1 * homogeneity + \\\n",
    "           lambda2 * np.exp(-dominant_brier) + \\\n",
    "           lambda3 * (1 - np.exp(-non_dominant_brier))\n",
    "\n",
    "# Calculate scores for each cluster\n",
    "cluster_0_score = calculate_cluster_score(\n",
    "    cluster_0_homogeneity,\n",
    "    cluster_0_brier,\n",
    "    cluster_0_dominant\n",
    ")\n",
    "\n",
    "cluster_1_score = calculate_cluster_score(\n",
    "    cluster_1_homogeneity,\n",
    "    cluster_1_brier,\n",
    "    cluster_1_dominant\n",
    ")\n",
    "\n",
    "print(\"\\nCluster Scores:\")\n",
    "print(f\"Cluster 0 Score: {cluster_0_score:.4f}\")\n",
    "print(f\"Cluster 1 Score: {cluster_1_score:.4f}\")\n",
    "print(f\"Cluster with higher score: {0 if cluster_0_score > cluster_1_score else 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the distance heatmaps for each sample in the dataset. We utilize this information to select the most relevant patch sections for the shortcut detection and naming part afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to revert the normalization transform\n",
    "def revert_transform(tensor):\n",
    "    # Convert tensor to numpy array\n",
    "    img = tensor.cpu().numpy()\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    # Reverse the normalization\n",
    "    img = img * np.array(std_ds) + np.array(mean_ds)\n",
    "    # Clip values to be between 0 and 1\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    return torch.tensor(img)\n",
    "\n",
    "NUM_SELECTED = 20\n",
    "\n",
    "# Calculate distances to centroids\n",
    "distances = kmeans.transform(pca_results)\n",
    "\n",
    "# Get indices of 5 closest images to each centroid\n",
    "closest_indices = []\n",
    "for i in range(n_clusters):  # We have 2 clusters\n",
    "    cluster_distances = distances[:, i]\n",
    "    closest_indices.extend(cluster_distances.argsort()[:NUM_SELECTED])\n",
    "\n",
    "print(closest_indices)\n",
    "\n",
    "concatenated_dataset = ConcatDataset([val_dataset_wo_patches, val_dataset_w_patches])\n",
    "#conatenated_saliency_maps = np.concatenate([saliency_maps_wo_patches, saliency_maps_w_patches], axis=0)\n",
    "\n",
    "# Get the corresponding images\n",
    "closest_images = [concatenated_dataset[i][0] for i in closest_indices]\n",
    "\n",
    "# Convert the closest images to numpy arrays and revert the normalization\n",
    "closest_images_np = [revert_transform(img).numpy() for img in closest_images]\n",
    "\n",
    "# Create a figure with 2 rows and 10 columns\n",
    "fig, axes = plt.subplots(2, NUM_SELECTED, figsize=(25, 4))\n",
    "fig.suptitle('Images Closest to Cluster Centroids', fontsize=16)\n",
    "\n",
    "# Plot images for cluster 0\n",
    "for i in range(NUM_SELECTED):\n",
    "    axes[0, i].imshow(closest_images_np[i])\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Plot images for cluster 1\n",
    "for i in range(NUM_SELECTED):\n",
    "    axes[1, i].imshow(closest_images_np[i+NUM_SELECTED])\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "output_dir = f'outputs/{MODEL_NAME}_{seed}/cluster_images/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save images for cluster 0\n",
    "for i in range(NUM_SELECTED):\n",
    "    img = closest_images_np[i]\n",
    "    # Convert from float [0,1] to uint8 [0,255] if needed\n",
    "    if img.dtype == np.float32 or img.dtype == np.float64:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "    plt.imsave(os.path.join(output_dir, f'cluster0_sample_{i}.png'), img)\n",
    "\n",
    "# Save images for cluster 1  \n",
    "for i in range(NUM_SELECTED):\n",
    "    img = closest_images_np[i+NUM_SELECTED]\n",
    "    # Convert from float [0,1] to uint8 [0,255] if needed\n",
    "    if img.dtype == np.float32 or img.dtype == np.float64:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "    plt.imsave(os.path.join(output_dir, f'cluster1_sample_{i}.png'), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = np.concatenate([keys_wo_patches, keys_w_patches], axis=0)\n",
    "\n",
    "output_base = f\"outputs/{MODEL_NAME}_{seed}\"\n",
    "\n",
    "# Calculate the mean of each cluster's keys\n",
    "cluster_0_keys = keys[kmeans.labels_ == 0]\n",
    "cluster_1_keys = keys[kmeans.labels_ == 1]\n",
    "cluster_0_mean = np.mean(cluster_0_keys, axis=0)\n",
    "cluster_1_mean = np.mean(cluster_1_keys, axis=0)\n",
    "\n",
    "\n",
    "# Get the indices of the NUM_SELECTED closest samples for each cluster\n",
    "cluster_0_indices = closest_indices[:NUM_SELECTED]\n",
    "cluster_1_indices = closest_indices[NUM_SELECTED:]\n",
    "\n",
    "\n",
    "# Calculate Euclidean distances\n",
    "distances = []\n",
    "for idx in cluster_0_indices:\n",
    "    distance_for_sample = []\n",
    "    for key_idx in range(196):  # Iterate over each of the 196 keys\n",
    "        distance_to_cluster_1 = np.linalg.norm(keys[idx][key_idx] - cluster_1_mean[key_idx]) # could be key_idx instead of mean\n",
    "        distance_for_sample.append(distance_to_cluster_1)\n",
    "    distances.append(distance_for_sample)   \n",
    "\n",
    "for idx in cluster_1_indices:\n",
    "    distance_for_sample = []\n",
    "    for key_idx in range(196):  # Iterate over each of the 196 keys\n",
    "        distance_to_cluster_0 = np.linalg.norm(keys[idx][key_idx] - cluster_0_mean[key_idx]) # could be key_idx instead of mean\n",
    "        distance_for_sample.append(distance_to_cluster_0)\n",
    "    distances.append(distance_for_sample)\n",
    "\n",
    "# Reshape distances for heatmap\n",
    "distances_matrix = np.array(distances).reshape(2*NUM_SELECTED, 196)\n",
    "\n",
    "# normalize distances_matrix to be between 0 and 1\n",
    "distances_matrix = (distances_matrix - distances_matrix.min()) / (distances_matrix.max() - distances_matrix.min())\n",
    "\n",
    "# Create a figure with subplots for each sample\n",
    "fig, axes = plt.subplots(2, NUM_SELECTED, figsize=(15, 5))\n",
    "fig.suptitle('Distance Heatmaps for Each Sample', fontsize=16)\n",
    "\n",
    "# Iterate over each sample\n",
    "for i in range(2*NUM_SELECTED):\n",
    "    row = i // NUM_SELECTED\n",
    "    col = i % NUM_SELECTED\n",
    "    \n",
    "    # Reshape the distances for this sample into a 14x14 grid\n",
    "    heatmap_data = distances_matrix[i].reshape(14, 14) \n",
    "  \n",
    "    # Save heatmap\n",
    "    cluster = 'cluster_0' if i < NUM_SELECTED else 'cluster_1'\n",
    "    sample_num = i % NUM_SELECTED + 1\n",
    "    heatmap_dir = f\"{output_base}/cluster_heatmaps/{cluster}\"\n",
    "    os.makedirs(heatmap_dir, exist_ok=True)\n",
    "\n",
    "    # Upsample heatmap to 256x256 using bicubic interpolation\n",
    "    upscaled_heatmap = cv2.resize(heatmap_data, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "    plt.imsave(f\"{heatmap_dir}/sample_{sample_num}_heatmap.png\", upscaled_heatmap, cmap='hot', vmin=0, vmax=1)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = axes[row, col].imshow(heatmap_data, cmap='hot', interpolation='nearest', vmin=0, vmax=1)\n",
    "    axes[row, col].axis('off') \n",
    "\n",
    "    # Set title for each subplot\n",
    "    if row == 0:\n",
    "        axes[row, col].set_title(f'Cluster 0\\nSample {col+1}')\n",
    "    else:\n",
    "        axes[row, col].set_title(f'Cluster 1\\nSample {col+1}')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to accommodate suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract the patches that we will use for the shortcut detection and naming part. We extract patches that have the highest activation in the heatmap and then we extract the surrounding patches to get a better understanding of the shortcut for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for high activation patches\n",
    "for i in range(2*NUM_SELECTED):\n",
    "    cluster = 'cluster_0' if i < NUM_SELECTED else 'cluster_1'\n",
    "    sample_dir = f\"{output_base}/{cluster}/sample_{i%NUM_SELECTED+1}/high_activations\"\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# For each image\n",
    "for i in range(2*NUM_SELECTED):\n",
    "    # Get image and its heatmap\n",
    "    img = closest_images_np[i]\n",
    "    heatmap = distances_matrix[i].reshape(14, 14)\n",
    "    \n",
    "    # Get index of highest activation\n",
    "    max_flat_idx = np.argmax(heatmap.flatten())\n",
    "    max_y, max_x = np.unravel_index(max_flat_idx, (14,14))\n",
    "    \n",
    "    cluster = 'cluster_0' if i < NUM_SELECTED else 'cluster_1'\n",
    "    sample_num = i % NUM_SELECTED + 1\n",
    "    \n",
    "    # Create grid centered on highest activation\n",
    "    patch_size = 16\n",
    "    grid_size = 6\n",
    "    \n",
    "    # Calculate starting positions in heatmap coordinates\n",
    "    start_y = max_y - grid_size//2 + 1\n",
    "    start_x = max_x - grid_size//2 + 1\n",
    "    \n",
    "    # Adjust grid size and starting positions if needed to stay in bounds\n",
    "    actual_start_y = max(0, start_y)\n",
    "    actual_start_x = max(0, start_x)\n",
    "    actual_end_y = min(14, start_y + grid_size)\n",
    "    actual_end_x = min(14, start_x + grid_size)\n",
    "    actual_grid_height = actual_end_y - actual_start_y\n",
    "    actual_grid_width = actual_end_x - actual_start_x\n",
    "        \n",
    "    combined_patch = np.zeros((patch_size * actual_grid_height, patch_size * actual_grid_width, 3))\n",
    "    \n",
    "    # Extract and place grid of patches around highest activation\n",
    "    for dy in range(actual_grid_height):\n",
    "        for dx in range(actual_grid_width):\n",
    "            y = actual_start_y + dy\n",
    "            x = actual_start_x + dx\n",
    "            \n",
    "            # Convert to image coordinates\n",
    "            img_y = y * patch_size\n",
    "            img_x = x * patch_size\n",
    "            \n",
    "            # Extract patch\n",
    "            patch = img[img_y:img_y+patch_size, img_x:img_x+patch_size]\n",
    "            \n",
    "            # Place patch in grid\n",
    "            grid_y = dy * patch_size\n",
    "            grid_x = dx * patch_size\n",
    "            combined_patch[grid_y:grid_y+patch_size, grid_x:grid_x+patch_size] = patch\n",
    "    \n",
    "    # Save combined patch grid\n",
    "    patch_path = f\"{output_base}/{cluster}/sample_{sample_num}/high_activations/centered_patch_grid.png\"\n",
    "    plt.imsave(patch_path, combined_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ the Replicate API to call the LLM models. We use the LLaVa-13B model for the captioning and the Mixtral-8x7B and Llama-3.1-8B and Llama-3.1-70B models for the shortcut detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_files = []\n",
    "\n",
    "for cluster in ['cluster_0', 'cluster_1']:\n",
    "    for sample_num in range(NUM_SELECTED):\n",
    "        patch_files.append(f\"{output_base}/{cluster}/sample_{sample_num}/high_activations/centered_patch_grid.png\")\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"<INSERT TOKEN HERE>\"\n",
    "api = replicate.Client(api_token=os.environ[\"REPLICATE_API_TOKEN\"])\n",
    "\n",
    "caption_models = [(\"yorickvp/llava-13b:b5f6212d032508382d61ff00469ddda3e32fd8a0e75dc39d8a4191bb742157fb\", \"llava-13b\")]\n",
    "\n",
    "# Captioning via the LLaVa-13B model\n",
    "for cap_mod in caption_models:\n",
    "    cluster_descriptions = []\n",
    "    \n",
    "    for cluster in ['cluster_0', 'cluster_1']:\n",
    "        descriptions = []\n",
    "        start_idx = 0 if cluster == 'cluster_0' else NUM_SELECTED\n",
    "        \n",
    "        for i in range(start_idx, start_idx + NUM_SELECTED):\n",
    "            output = api.run(\n",
    "                cap_mod[0],\n",
    "                input={\"image\": open(patch_files[i],'rb'), \"prompt\": \"What is in this picture? Describe in a few words.\"}\n",
    "            )\n",
    "            descriptions.append(\"\".join(output))\n",
    "        \n",
    "        cluster_descriptions.append(descriptions)\n",
    "        \n",
    "        # Create descriptions directory and save\n",
    "        os.makedirs(f\"{output_base}/{cluster}/descriptions\", exist_ok=True)\n",
    "        with open(f\"{output_base}/{cluster}/descriptions/{cap_mod[1]}_descriptions.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(descriptions))\n",
    "\n",
    "shortcut_models = [(\"meta/meta-llama-3-8b-instruct\", \"llama8b\"), (\"meta/meta-llama-3-70b-instruct\", \"llama70b\")]\n",
    "\n",
    "# Calling the LLama Models\n",
    "for short_mod in shortcut_models:\n",
    "    for cluster in ['cluster_0', 'cluster_1']:\n",
    "        cluster_idx = 0 if cluster == 'cluster_0' else 1\n",
    "        input = {\n",
    "            \"prompt\": f\"I extracted patches from images in my dataset where my model seems to focus on the most. I let an LLM caption these images for you. I am searching for potential shortcuts in the dataset. Can you identify one or more possible shortcuts in this dataset? Describe it in one sentence (only!) and pick the most significant. No other explanations are needed. Descriptions: \\n\" + \"\".join(cluster_descriptions[cluster_idx]),\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        }\n",
    "\n",
    "        output = api.run(\n",
    "            short_mod[0],\n",
    "            input=input\n",
    "        )\n",
    "\n",
    "        text = \"\".join(output)\n",
    "        \n",
    "        os.makedirs(f\"{output_base}/{cluster}/descriptions\", exist_ok=True)\n",
    "        with open(f\"{output_base}/{cluster}/descriptions/{short_mod[1]}_shortcut.txt\", \"w\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "# Mixtral model\n",
    "for cluster in ['cluster_0', 'cluster_1']:\n",
    "    cluster_idx = 0 if cluster == 'cluster_0' else 1\n",
    "    input = {\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"prompt\": f\"I extracted patches from images in my dataset where my model seems to focus on the most. I let an LLM caption these images for you. I am searching for potential shortcuts in the dataset. Can you identify one or more possible shortcuts in this dataset? Describe it in one sentence (only!) and pick the most significant. No other explanations are needed. If there isn't any shortcut obvious to you then just say so. Descriptions: \\n\" + \"\".join(cluster_descriptions[cluster_idx]),\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"prompt_template\": \"<s>[INST] {prompt} [/INST] \"\n",
    "    }\n",
    "\n",
    "    output = api.run(\n",
    "        \"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "        input=input\n",
    "    )\n",
    "\n",
    "    text = \"\".join(output)\n",
    "\n",
    "    os.makedirs(f\"{output_base}/{cluster}/descriptions\", exist_ok=True)\n",
    "    with open(f\"{output_base}/{cluster}/descriptions/mixtral_shortcut.txt\", \"w\") as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base = f\"outputs/{MODEL_NAME}_{seed}\"\n",
    "\n",
    "# Create directories for patches\n",
    "for i in range(2*NUM_SELECTED):\n",
    "    cluster = 'cluster_0' if i < NUM_SELECTED else 'cluster_1'\n",
    "    sample_dir = f\"{output_base}/{cluster}/sample_{i%NUM_SELECTED+1}/patches\"\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# For each image\n",
    "for i in range(2*NUM_SELECTED):\n",
    "    # Get image and its heatmap\n",
    "    img = closest_images_np[i]\n",
    "    heatmap = distances_matrix[i].reshape(14, 14)\n",
    "    \n",
    "    # Get indices of top 5 activations\n",
    "    top_5_flat_indices = np.argpartition(heatmap.flatten(), -5)[-5:]\n",
    "    top_5_positions = [np.unravel_index(idx, (14,14)) for idx in top_5_flat_indices]\n",
    "    \n",
    "    cluster = 'cluster_0' if i < NUM_SELECTED else 'cluster_1'\n",
    "    sample_num = i % NUM_SELECTED + 1\n",
    "    \n",
    "    # Extract and save patches for each top activation\n",
    "    for patch_idx, (y, x) in enumerate(top_5_positions):\n",
    "        # Convert heatmap coordinates to image coordinates\n",
    "        patch_size = 16\n",
    "        img_y = y * patch_size\n",
    "        img_x = x * patch_size\n",
    "        \n",
    "        # Extract patch\n",
    "        patch = img[img_y:img_y+patch_size, img_x:img_x+patch_size]\n",
    "        \n",
    "        # Save patch\n",
    "        patch_path = f\"{output_base}/{cluster}/sample_{sample_num}/patches/patch_{patch_idx+1}.png\"\n",
    "        plt.imsave(patch_path, patch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parts are only used for visualization and qualitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if they don't exist\n",
    "import os\n",
    "\n",
    "output_base = f\"outputs/{MODEL_NAME}_{seed}\"\n",
    "os.makedirs(f\"{output_base}/cluster_0\", exist_ok=True)\n",
    "os.makedirs(f\"{output_base}/cluster_1\", exist_ok=True)\n",
    "\n",
    "def plot_and_save_heatmaps_and_patches(closest_images_np, distances_matrix, NUM_SELECTED, grid_size=5):\n",
    "    # Create a figure with subplots for visualization\n",
    "    fig, axes = plt.subplots(4, NUM_SELECTED, figsize=(20, 8))\n",
    "    fig.suptitle(f'Distance Heatmaps and {grid_size}x{grid_size} Surrounding Patches', fontsize=16)\n",
    "    \n",
    "    # Iterate over each sample\n",
    "    for i in range(2*NUM_SELECTED):\n",
    "        row = (i // NUM_SELECTED) * 2\n",
    "        col = i % NUM_SELECTED\n",
    "        cluster = 0 if row == 0 else 1\n",
    "        \n",
    "        # Get the original image\n",
    "        img = closest_images_np[i]\n",
    "        \n",
    "        # Reshape the distances for this sample into a 14x14 grid\n",
    "        heatmap_data = distances_matrix[i].reshape(14, 14)\n",
    "        \n",
    "        # Find the position of maximum activation\n",
    "        max_pos = np.unravel_index(np.argmax(heatmap_data), heatmap_data.shape)\n",
    "        \n",
    "        # Calculate the corresponding position in the original image (224x224)\n",
    "        patch_size = 16\n",
    "        center_y = max_pos[0] * patch_size\n",
    "        center_x = max_pos[1] * patch_size\n",
    "        \n",
    "        # Collect valid patches and their positions\n",
    "        valid_patches = []\n",
    "        min_y, max_y = float('inf'), -float('inf')\n",
    "        min_x, max_x = float('inf'), -float('inf')\n",
    "        \n",
    "        offset = grid_size // 2\n",
    "        for dy in range(-offset, offset + 1):\n",
    "            for dx in range(-offset, offset + 1):\n",
    "                y = center_y + (dy * patch_size)\n",
    "                x = center_x + (dx * patch_size)\n",
    "                \n",
    "                # Check boundaries\n",
    "                if (0 <= y < img.shape[0]-patch_size and \n",
    "                    0 <= x < img.shape[1]-patch_size):\n",
    "                    patch = img[y:y+patch_size, x:x+patch_size]\n",
    "                    valid_patches.append((dy+offset, dx+offset, patch))\n",
    "                    min_y = min(min_y, dy+offset)\n",
    "                    max_y = max(max_y, dy+offset)\n",
    "                    min_x = min(min_x, dx+offset)\n",
    "                    max_x = max(max_x, dx+offset)\n",
    "        \n",
    "        if valid_patches:\n",
    "            # Create composite patch of exact size needed\n",
    "            height = (max_y - min_y + 1) * patch_size\n",
    "            width = (max_x - min_x + 1) * patch_size\n",
    "            composite_patch = np.zeros((height, width, 3))\n",
    "            \n",
    "            # Place patches in their relative positions\n",
    "            for py, px, patch in valid_patches:\n",
    "                y_pos = (py - min_y) * patch_size\n",
    "                x_pos = (px - min_x) * patch_size\n",
    "                composite_patch[y_pos:y_pos+patch_size, x_pos:x_pos+patch_size] = patch\n",
    "            \n",
    "            # Save the composite patch without white space\n",
    "            plt.imsave(\n",
    "                f\"{output_base}/cluster_{cluster}/sample_{col+1}_patches.png\",\n",
    "                composite_patch\n",
    "            )\n",
    "            \n",
    "            # For visualization in the notebook\n",
    "            im = axes[row, col].imshow(heatmap_data, cmap='hot', interpolation='nearest', vmin=0, vmax=1)\n",
    "            axes[row, col].axis('off')\n",
    "            \n",
    "            axes[row+1, col].imshow(composite_patch)\n",
    "            axes[row+1, col].axis('off')\n",
    "            \n",
    "            # Set titles\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(f'Cluster 0\\nSample {col+1}')\n",
    "            else:\n",
    "                axes[row, col].set_title(f'Cluster 1\\nSample {col+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_and_save_heatmaps_and_patches(closest_images_np, distances_matrix, NUM_SELECTED, grid_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the 100 most distant keys for each cluster and the 100 lowest keys for each cluster. This is used for the shortcut detection using a KNN classifier as described in section 3.4.1 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 100 most distant keys for each cluster\n",
    "top_100_keys_cluster_0 = []\n",
    "top_100_keys_cluster_1 = []\n",
    "\n",
    "lowest_100_keys_cluster_0 = []\n",
    "lowest_100_keys_cluster_1 = []\n",
    "\n",
    "top_100_img_indices_c0 = []\n",
    "top_100_img_indices_c1 = []\n",
    "\n",
    "NUM_SELECTED_PATCHES = 200 \n",
    "\n",
    "# For cluster 0\n",
    "cluster_0_distances = distances_matrix[:NUM_SELECTED]\n",
    "top_100_indices_cluster_0 = np.argsort(cluster_0_distances.flatten())[-NUM_SELECTED_PATCHES:]\n",
    "for idx in top_100_indices_cluster_0:\n",
    "    sample_idx = idx // 196\n",
    "    key_idx = idx % 196\n",
    "    top_100_img_indices_c0.append((sample_idx, key_idx))\n",
    "    top_100_keys_cluster_0.append(keys[cluster_0_indices[sample_idx]][key_idx])\n",
    "\n",
    "# For cluster 1\n",
    "cluster_1_distances = distances_matrix[NUM_SELECTED:]\n",
    "top_100_indices_cluster_1 = np.argsort(cluster_1_distances.flatten())[-NUM_SELECTED_PATCHES:]\n",
    "for idx in top_100_indices_cluster_1:\n",
    "    sample_idx = idx // 196\n",
    "    key_idx = idx % 196\n",
    "    top_100_img_indices_c1.append((sample_idx, key_idx))\n",
    "    top_100_keys_cluster_1.append(keys[cluster_1_indices[sample_idx]][key_idx])\n",
    "\n",
    "# Get the 100 lowest keys for each cluster\n",
    "cluster_0_distances = distances_matrix[:NUM_SELECTED]\n",
    "lowest_100_indices_cluster_0 = np.argsort(cluster_0_distances.flatten())[:NUM_SELECTED_PATCHES]\n",
    "for idx in lowest_100_indices_cluster_0:\n",
    "    sample_idx = idx // 196\n",
    "    key_idx = idx % 196\n",
    "    lowest_100_keys_cluster_0.append(keys[cluster_0_indices[sample_idx]][key_idx])\n",
    "\n",
    "cluster_1_distances = distances_matrix[NUM_SELECTED:]\n",
    "lowest_100_indices_cluster_1 = np.argsort(cluster_1_distances.flatten())[:NUM_SELECTED_PATCHES]\n",
    "for idx in lowest_100_indices_cluster_1:\n",
    "    sample_idx = idx // 196\n",
    "    key_idx = idx % 196\n",
    "    lowest_100_keys_cluster_1.append(keys[cluster_1_indices[sample_idx]][key_idx])\n",
    "\n",
    "# Convert to numpy arrays and reshape\n",
    "top_100_keys_cluster_0 = np.array(top_100_keys_cluster_0).reshape(NUM_SELECTED_PATCHES, 64)\n",
    "top_100_keys_cluster_1 = np.array(top_100_keys_cluster_1).reshape(NUM_SELECTED_PATCHES, 64)\n",
    "\n",
    "print(\"Shape of top 100 keys for cluster 0:\", top_100_keys_cluster_0.shape)\n",
    "print(\"Shape of top 100 keys for cluster 1:\", top_100_keys_cluster_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a KNN classifier to detect shortcuts during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([lowest_100_keys_cluster_0, top_100_keys_cluster_1], axis=0)\n",
    "y = np.concatenate([np.zeros(NUM_SELECTED_PATCHES), np.ones(NUM_SELECTED_PATCHES)], axis=0)\n",
    "\n",
    "# Initialize and train the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_out_model = Extract(model, node_out=\"encoder.dropout\")\n",
    "conv_in_model = Extract(model, node_in=\"encoder.dropout\", node_out=\"heads.head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is helper code to get the surrounding patches for the shortcut detection and to have an inference function that utilizes the KNN classifier to detect shortcuts during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_surrounding_indices(index, grid_size):\n",
    "    patch_size = 16\n",
    "    num_patches = grid_size // patch_size\n",
    "    row = index // num_patches\n",
    "    col = index % num_patches\n",
    "    \n",
    "    surrounding_indices = []\n",
    "    \n",
    "    # Define the possible directions (top, bottom, left, right, and diagonals)\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "    \n",
    "    for dr, dc in directions:\n",
    "        new_row = row + dr\n",
    "        new_col = col + dc\n",
    "        \n",
    "        # Check if the new coordinates are within bounds\n",
    "        if 0 <= new_row < num_patches and 0 <= new_col < num_patches:\n",
    "            surrounding_index = new_row * num_patches + new_col\n",
    "            surrounding_indices.append(surrounding_index)\n",
    "    \n",
    "    return surrounding_indices\n",
    "\n",
    "def add_surrounding_patches(indices, grid_size=224):\n",
    "    # Convert the list of indices to a set for efficient look-up\n",
    "    indices_set = set(indices)\n",
    "    \n",
    "    new_indices = set(indices)\n",
    "    \n",
    "    for index in indices:\n",
    "        surrounding_indices = get_surrounding_indices(index, grid_size)\n",
    "        for surrounding_index in surrounding_indices:\n",
    "            if surrounding_index not in indices_set:\n",
    "                new_indices.add(surrounding_index)\n",
    "    \n",
    "    return list(new_indices)\n",
    "\n",
    "def get_mask_from_indices(indices):\n",
    "    mask = torch.ones(197, dtype=bool)\n",
    "    mask[indices] = False\n",
    "    return mask\n",
    "\n",
    "def inference_for_loader(loader):\n",
    "    correct_abilation = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ablated = 0\n",
    "    correct_images = []\n",
    "\n",
    "    for img, target in tqdm(loader):\n",
    "        all_patches = conv_out_model(img.to(device))[0, 1:, :].detach().cpu().numpy()\n",
    "\n",
    "        ln_1 = torch.tensor(all_patches.reshape(196, -1)).to(device)\n",
    "        k = ln_1 @ W_K.T\n",
    "\n",
    "        k = k.reshape(196, 12, 64)\n",
    "        all_keys = k.mean(dim=1).detach().cpu()\n",
    "\n",
    "        preds = knn.predict(all_keys)\n",
    "\n",
    "        indices = np.where(preds == 1)[0] \n",
    "\n",
    "        indices = np.array(add_surrounding_patches(indices))\n",
    "\n",
    "        indices += 1\n",
    "\n",
    "        if len(indices) > 0:\n",
    "            ablated += 1\n",
    "\n",
    "        mask = get_mask_from_indices(indices)\n",
    "        \n",
    "        x = img.to(device)\n",
    "        encoder = conv_out_model(x)\n",
    "        enc = encoder[:, mask, :]\n",
    "\n",
    "        pred_abilation = F.softmax(conv_in_model(x, enc), dim=1).argmax().item()\n",
    "        pred = F.softmax(model(x), dim=1).argmax().item()\n",
    "\n",
    "        if pred == target.item():\n",
    "            correct += 1\n",
    "\n",
    "        if pred_abilation == target.item():\n",
    "            correct_abilation += 1\n",
    "            \n",
    "        if pred != pred_abilation:\n",
    "            correct_images.append(img)\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    return correct, correct_abilation, total, correct_images, ablated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform the inference on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_patches_cls_0_loader = torch.utils.data.DataLoader(test_w_patches_cls_0, batch_size=1, shuffle=False)\n",
    "test_w_patches_cls_1_loader = torch.utils.data.DataLoader(test_w_patches_cls_1, batch_size=1, shuffle=False)\n",
    "test_wo_patches_cls_0_loader = torch.utils.data.DataLoader(test_wo_patches_cls_0, batch_size=1, shuffle=False)\n",
    "test_wo_patches_cls_1_loader = torch.utils.data.DataLoader(test_wo_patches_cls_1, batch_size=1, shuffle=False)\n",
    "\n",
    "corrects, correct_abilations, totals, ablateds = [], [], [], []\n",
    "\n",
    "correct, correct_abilation, total, correct_images, ablated = inference_for_loader(test_w_patches_cls_0_loader)\n",
    "print(correct, correct_abilation, total, ablated)\n",
    "corrects.append(correct)\n",
    "correct_abilations.append(correct_abilation)\n",
    "totals.append(total)\n",
    "ablateds.append(ablated)\n",
    "\n",
    "correct, correct_abilation, total, correct_images, ablated = inference_for_loader(test_w_patches_cls_1_loader)\n",
    "print(correct, correct_abilation, total, ablated)\n",
    "corrects.append(correct)\n",
    "correct_abilations.append(correct_abilation)\n",
    "totals.append(total)\n",
    "ablateds.append(ablated)\n",
    "\n",
    "correct, correct_abilation, total, correct_images, ablated = inference_for_loader(test_wo_patches_cls_0_loader)\n",
    "print(correct, correct_abilation, total, ablated)\n",
    "corrects.append(correct)\n",
    "correct_abilations.append(correct_abilation)\n",
    "totals.append(total)\n",
    "ablateds.append(ablated)\n",
    "\n",
    "correct, correct_abilation, total, correct_images, ablated = inference_for_loader(test_wo_patches_cls_1_loader)\n",
    "print(correct, correct_abilation, total, ablated)\n",
    "corrects.append(correct)\n",
    "correct_abilations.append(correct_abilation)\n",
    "totals.append(total)\n",
    "ablateds.append(ablated)\n",
    "\n",
    "import pandas as pd\n",
    "res = pd.DataFrame([corrects, correct_abilations, totals, ablateds], columns=[\"Malignant w/ Patch\", \"Benign w/ Patch\", \"Malignant w/o Patch\", \"Benign w/o Patch\"], index=[\"W Patches Benign\", \"W Patches Malignant\", \"Wo Patches Benign\", \"Wo Patches Malignant\"])\n",
    "res.to_csv(f\"outputs/{MODEL_NAME}_{seed}/results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next parts helps us understand how many images have at least one ablated patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_shortcut_cls_0 = []\n",
    "val_shortcut_cls_1 = []\n",
    "val_non_shortcut_cls_0 = []\n",
    "val_non_shortcut_cls_1 = []\n",
    "\n",
    "for i in tqdm(range(len(concatenated_dataset))):\n",
    "    all_patches = conv_out_model(concatenated_dataset[i][0].unsqueeze(0).to(device))[0, 1:, :].detach().cpu().numpy()\n",
    "\n",
    "    ln_1 = torch.tensor(all_patches.reshape(196, -1)).to(device)\n",
    "    k = ln_1 @ W_K.T\n",
    "\n",
    "    k = k.reshape(196, 12, 64)\n",
    "    all_keys = k.mean(dim=1).detach().cpu()\n",
    "\n",
    "    preds = knn.predict(all_keys)\n",
    "    if preds.sum() > 1 and concatenated_dataset[i][1] == 0:\n",
    "        val_shortcut_cls_0.append(i)\n",
    "    elif preds.sum() > 1 and concatenated_dataset[i][1] == 1:\n",
    "        val_shortcut_cls_1.append(i)\n",
    "    elif preds.sum() == 0 and concatenated_dataset[i][1] == 0:\n",
    "        val_non_shortcut_cls_0.append(i)\n",
    "    elif preds.sum() == 0 and concatenated_dataset[i][1] == 1:\n",
    "        val_non_shortcut_cls_1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(val_shortcut_cls_0), len(val_shortcut_cls_1), len(val_non_shortcut_cls_0), len(val_non_shortcut_cls_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now based on this we follow DFR to create equally weighted subsets for last layer retraining. We orient ourselves on the shortest list to create a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# get the minimum length of the four lists\n",
    "min_length = min(len(val_shortcut_cls_0), len(val_shortcut_cls_1), len(val_non_shortcut_cls_0), len(val_non_shortcut_cls_1))\n",
    "\n",
    "# randomly sample the same number of images from each list\n",
    "sampled_val_shortcut_cls_0 = random.sample(val_shortcut_cls_0, min_length)\n",
    "sampled_val_shortcut_cls_1 = random.sample(val_shortcut_cls_1, min_length)\n",
    "sampled_val_non_shortcut_cls_0 = random.sample(val_non_shortcut_cls_0, min_length)\n",
    "sampled_val_non_shortcut_cls_1 = random.sample(val_non_shortcut_cls_1, min_length)\n",
    "\n",
    "# create subset of concatenated_dataset\n",
    "subset = torch.utils.data.Subset(concatenated_dataset, sampled_val_shortcut_cls_0 + sampled_val_shortcut_cls_1 + sampled_val_non_shortcut_cls_0 + sampled_val_non_shortcut_cls_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(subset, batch_size=1, shuffle=False)\n",
    "\n",
    "train_images = []\n",
    "train_encs = []\n",
    "lbls = []\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "selected_images = []\n",
    "\n",
    "idx = 0\n",
    "for epoch in range(num_epochs):\n",
    "    conv_in_model.train()\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            lbls.append(labels.item())\n",
    "            \n",
    "            out = conv_out_model(images.to(device)).detach().cpu().numpy()\n",
    "\n",
    "            all_patches = out[0, 1:, :]\n",
    "\n",
    "            ln_1 = torch.tensor(all_patches.reshape(196, -1)).to(device)\n",
    "            k = ln_1 @ W_K.T\n",
    "\n",
    "            k = k.reshape(196, 12, 64)\n",
    "            all_keys = k.mean(dim=1).detach().cpu()\n",
    "\n",
    "            means = knn.predict(all_keys.reshape(196, -1))\n",
    "            indices = np.where(means > 0)[0] \n",
    "            indices = np.array(add_surrounding_patches(indices))\n",
    "\n",
    "            indices += 1\n",
    "\n",
    "            mask = get_mask_from_indices(indices)\n",
    "            \n",
    "            x = images.to(device)\n",
    "            enc = out[:, mask, :]\n",
    "\n",
    "            train_encs.append(enc)\n",
    "            train_images.append(x.cpu())\n",
    "\n",
    "            selected_images.append(idx)\n",
    "\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a custom dataset to train the last layer of the model and then freezed all other layers in the model. Since we are using ablation during this process we are using pytorch surgeons extract function to extract the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Assume train_encs is provided as input\n",
    "sequences = [torch.tensor(arr).squeeze().cpu() for arr in train_encs]\n",
    "\n",
    "padded_encs = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "# Assume train_images and dataset.targets are provided as input\n",
    "train_images = torch.stack(train_images).squeeze().cpu()\n",
    "trgts = torch.tensor(lbls)\n",
    "\n",
    "# Custom dataset for train_images, full_batch, and trgts\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, encs, targets):\n",
    "        self.images = images\n",
    "        self.encs = encs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.encs[idx], self.targets[idx]\n",
    "\n",
    "custom_dataset = CustomDataset(train_images, padded_encs, trgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(custom_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "conv_out_model = Extract(model, node_out=\"encoder.dropout\")\n",
    "conv_in_model = Extract(model, node_in=\"encoder.dropout\", node_out=\"heads.head\")\n",
    "\n",
    "# set every parameter to False but the last one in conv_in_model\n",
    "for name, param in conv_in_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "conv_in_model.heads.head.weight.requires_grad = True\n",
    "conv_in_model.heads.head.bias.requires_grad = True\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(conv_in_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We traing the last-layer of the model for a given nuber of epochs and test the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "conv_in_model.train()\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_img, batch_enc, batch_trgts in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = conv_in_model(batch_img.to(device), batch_enc.to(device))\n",
    "        loss = criterion(outputs, batch_trgts.to(device))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "conv_in_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects, correct_abilations, totals, ablateds = [], [], [], []\n",
    "\n",
    "correct, correct_abilation, total, correct_images, ablated = inference_for_loader(test_w_patches_cls_0_loader)\n",
    "print(correct, correct_abilation, total, ablated)\n",
    "corrects.append(correct)\n",
    "correct_abilations.append(correct_abilation)\n",
    "totals.append(total)\n",
    "ablateds.append(ablated)\n",
    "\n",
    "correct, correct_abilation, total, correct_images, ablated = inference_for_loader(test_w_patches_cls_1_loader)\n",
    "print(correct, correct_abilation, total, ablated)\n",
    "corrects.append(correct)\n",
    "correct_abilations.append(correct_abilation)\n",
    "totals.append(total)\n",
    "ablateds.append(ablated)\n",
    "\n",
    "correct, correct_abilation, total, correct_images, ablated = inference_for_loader(test_wo_patches_cls_0_loader)\n",
    "print(correct, correct_abilation, total, ablated)\n",
    "corrects.append(correct)\n",
    "correct_abilations.append(correct_abilation)\n",
    "totals.append(total)\n",
    "ablateds.append(ablated)\n",
    "\n",
    "correct, correct_abilation, total, correct_images, ablated = inference_for_loader(test_wo_patches_cls_1_loader)\n",
    "print(correct, correct_abilation, total, ablated)\n",
    "corrects.append(correct)\n",
    "correct_abilations.append(correct_abilation)\n",
    "totals.append(total)\n",
    "ablateds.append(ablated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res = pd.DataFrame([corrects, correct_abilations, totals, ablateds], columns=[\"Malignant w/ Patch\", \"Benign w/ Patch\", \"Malignant w/o Patch\", \"Benign w/o Patch\"], index=[\"W Patches Benign\", \"W Patches Malignant\", \"Wo Patches Benign\", \"Wo Patches Malignant\"])\n",
    "res.to_csv(f\"outputs/{MODEL_NAME}_{seed}/results_retraining_val.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
